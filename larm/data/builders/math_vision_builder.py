import os
import logging
from typing import Dict, List

from datasets import DatasetDict, load_dataset

from larm.data.builders.base_builder import BaseDatasetBuilder
from larm.data.interactions.singleturn_interaction import SingleTurnInteractionManager
from larm.common.registry import registry
from larm.data.envs.math_vision_env import MathVisionEnv


@registry.register_builder("math_vision")
class MathVisionBuilder(BaseDatasetBuilder):
    """
    Math Vision dataset builder.
    
    Expects preprocessed data generated by scripts/math_vision_process.py
    with fields: prompt, completion, solution, image_path
    
    Expected directory structure:
        data_path/
          train.json   # preprocessed samples with prompt, completion, solution, image_path
          valid.json
          test.json
    """

    DATASET_CONFIG_DICT = {
        "default": "configs/datasets/math_vision/default.yaml",
    }
    CACHE_PATH = None

    def _build_datasets(self) -> DatasetDict:
        """Build SFT/RL datasets from preprocessed JSON files.
        
        The preprocessing (download, split, format) should be done via:
            python scripts/math_vision_process.py --config configs/latent_memory/math_vision.yaml
        
        Data is always loaded from 'data/math_vision' directory.
        """
        # Always use data/math_vision as the data path (unified location for both SFT and GRPO)
        data_path = "data/math_vision"

        # logging.info(f"[Math_Vision] Using unified data path: {data_path}")

        # Load preprocessed splits
        data_files = {}
        for split_name in ["train", "valid", "test"]:
            json_path = os.path.join(data_path, f"{split_name}.json")
            if os.path.exists(json_path):
                data_files[split_name] = json_path

        if len(data_files) == 0:
            raise FileNotFoundError(
                f"No preprocessed data found in {data_path}. "
                f"Please run 'python scripts/math_vision_process.py --output_dir {data_path}' first."
            )

        # logging.info(f"[Math_Vision] Loading preprocessed data from {data_path}")
        # logging.info(f"[Math_Vision] Found splits: {list(data_files.keys())}")

        dataset_dict = load_dataset("json", data_files=data_files)

        # Apply max_sample if specified (only for training data with random sampling)
        # Try to get from full_config first (top-level), then from mode-specific config
        max_sample = None
        if hasattr(self, 'full_config') and self.full_config is not None:
            max_sample = self.full_config.get("max_sample", None)
        if max_sample is None:
            max_sample = self.config.get("max_sample", None)

        if max_sample is not None and max_sample > 0:
            logging.info(f"[Math_Vision] Applying max_sample={max_sample} with random sampling")
            # Only apply to training set
            if "train" in dataset_dict:
                original_size = len(dataset_dict["train"])
                if original_size > max_sample:
                    # Randomly shuffle and select max_sample samples
                    dataset_dict["train"] = dataset_dict["train"].shuffle(seed=42).select(range(max_sample))
                    logging.info(f"[Math_Vision] train: {original_size} -> {len(dataset_dict['train'])} samples (randomly sampled)")
                else:
                    logging.info(f"[Math_Vision] train: keeping all {original_size} samples (less than max_sample)")

        # Verify expected fields
        required_fields = ["prompt", "completion", "solution", "image_path"]
        for split_name, ds in dataset_dict.items():
            missing_keys = set(required_fields) - set(ds.column_names)
            if missing_keys:
                raise ValueError(
                    f"Split '{split_name}' is missing required fields: {missing_keys}. "
                    f"Please regenerate preprocessed data using scripts/math_vision_process.py"
                )
            # logging.info(f"[Math_Vision] {split_name}: {len(ds)} samples")

        # Adjust completion field based on mode
        # sft: use full solution text (completion field)
        # grpo: use extracted answer (solution field)
        if self.mode == "grpo":
            # logging.info("[Math_Vision] Mode: grpo - using solution field (answer only)")
            for split_name in dataset_dict.keys():
                # For grpo, replace completion with solution (empty completion, solution has answer)
                def use_solution(example):
                    example["completion"] = ""  # grpo generates from scratch
                    return example
                dataset_dict[split_name] = dataset_dict[split_name].map(use_solution)
        # else:
            # logging.info("[Math_Vision] Mode: sft - using completion field (full solution text)")

        keep_keys = self._keep_keys()
        for split_name in dataset_dict.keys():
            dataset_dict[split_name] = dataset_dict[split_name].select_columns(keep_keys)

        # Log example
        if "train" in dataset_dict and len(dataset_dict["train"]) > 0:
            example = dataset_dict["train"][0]
            # logging.info(f"[Math_Vision] Example after mode adjustment: {example}")

        return dataset_dict

    def _build_sft_datasets(self) -> DatasetDict:
        return self._build_datasets()

    def _build_rl_datasets(self) -> DatasetDict:
        return self._build_datasets()

    @classmethod
    def _keep_keys(cls) -> List[str]:
        """Required fields in preprocessed data."""
        return ["prompt", "completion", "solution", "image_path"]

    def get_env_cls(self):
        return MathVisionEnv

    def get_generation_manager_cls(self):
        return SingleTurnInteractionManager
