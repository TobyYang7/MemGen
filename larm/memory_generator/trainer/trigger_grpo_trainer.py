from trl import GRPOTrainer, GRPOConfig
from trl.models import unwrap_model_for_generation, create_reference_model   
from trl.data_utils import maybe_apply_chat_template 
from transformers import (
    PreTrainedModel, 
    PreTrainedTokenizerBase, 
    TrainerCallback
)
from peft import PeftConfig, get_peft_model

from typing import Union, Callable, Optional, Any
from contextlib import nullcontext
import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.utils.data import Dataset
import re
import copy
import gc
from accelerate.utils import gather_object

from larm.data.interactions.base_interaction import InteractionDataProto
from larm.data.utils.tensor_utils import TensorHelper, TensorConfig

from .utils import (
    nanstd, nanmax, nanmin
)
from ..memgen_model import LatentMemoryModel

RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]

class TriggerGRPOTrainer(GRPOTrainer):
    def __init__(
        self,
        model: LatentMemoryModel,
        processing_class: PreTrainedTokenizerBase,
        train_dataset: Dataset,
        eval_dataset: Dataset,
        reward_funcs: Union[RewardFunc, list[RewardFunc]],
        reward_processing_classes: Optional[Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]] = None,
        args: Optional[GRPOConfig] = None,
        callbacks: Optional[list[TrainerCallback]] = None,
        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),
        peft_config: Optional[PeftConfig] = None,
    ):        
        # NOTE - Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the
        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set
        # self.model_accepts_loss_kwargs to False to enable scaling.
        self.model_accepts_loss_kwargs = False

        super().__init__(
            model=model,
            args=args,
            reward_funcs=reward_funcs,
            reward_processing_classes=reward_processing_classes,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=processing_class,
            callbacks=callbacks,
            optimizers=optimizers,
            peft_config=peft_config
        )

        # If PEFT configuration is not provided, create a reference model based on the initial model.
        ref_model = create_reference_model(model.trigger)
        self.ref_model = self.accelerator.prepare_model(ref_model, evaluation_mode=True)
        self.tensor_fn = TensorHelper(TensorConfig(
            pad_token_id=self.processing_class.pad_token_id,
            max_prompt_length=self.max_prompt_length,
            max_obs_length=None,
            max_start_length=None
        ))
        
        # Initialize GRPO logging file
        import os
        self.grpo_log_file = os.path.join(args.output_dir, "grpo_logs.txt")
        os.makedirs(args.output_dir, exist_ok=True)
        # Create/clear the log file
        with open(self.grpo_log_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("GRPO Training Logs - TriggerGRPOTrainer\n")
            f.write("=" * 80 + "\n\n")
    
    def training_step(self, model, inputs, num_items_in_batch=None):
        """
        Perform a training step on a batch of inputs, with memory cleanup after each step.
        """
        # Call the parent class's training_step
        loss = super().training_step(model, inputs, num_items_in_batch)
        
        # Clear memory after each training step
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return loss
    
    def _set_signature_columns_if_needed(self):
        # NOTE - If `self.args.remove_unused_columns` is True, non-signature columns are removed.
        # By default, this method sets `self._signature_columns` to the model's expected inputs.
        # In LatentProcessorSFTTrainer, we preprocess data, so using the model's signature columns doesn't work.
        # Instead, we set them to the columns expected by the `training_step` method, hence the override.
        pass   

    def _get_per_token_logps(
        self, 
        model, 
        input_ids: torch.Tensor, 
        attention_mask: torch.Tensor, 
        augmentation_ids: torch.Tensor, 
        augmentation_mask: torch.Tensor
    ) -> torch.Tensor:
        prompt_len = attention_mask.size(1) - augmentation_mask.size(1)
        
        assert input_ids.size(1) == attention_mask.size(1)
        augmentation_logits = model(input_ids=input_ids, attention_mask=attention_mask)
        clipped_logits = augmentation_logits[:, prompt_len - 1 : -1]
        assert clipped_logits.shape[:-1] == augmentation_ids.shape == augmentation_mask.shape

        safe_augmentation_ids = augmentation_ids.clone().to(torch.int64)
        safe_augmentation_ids[augmentation_mask == 0] = 0
        log_probs = clipped_logits.log_softmax(dim=-1) 
        # output[i][j][k] = log_probs[i][j][safe_augmentation_mask[i][j][k]]
        per_token_logps = torch.gather(
            log_probs, dim=2, 
            index=safe_augmentation_ids.unsqueeze(2)
        ).squeeze(2) 
        
        per_token_logps[augmentation_mask == 0] = 0
        
        return per_token_logps
    
    def _generate_and_score_completions(
        self, inputs: list[dict[str, Union[torch.Tensor, Any]]]
    ) -> dict[str, Union[torch.Tensor, Any]]:
        
        device = self.accelerator.device
        mode = "train" if self.model.training else "eval"

        prompts = [x["prompt"] for x in inputs]
        invalid_augmentation_id = -100

        # modified: pop those keys for generation
        batch_gen_keys = []
        if "prompt" in inputs[0]:  # text-based raw prompt
            batch_gen_keys.append("prompt")
        if "tools_kwargs" in inputs[0]:  # tool-integrated     
            batch_gen_keys.append("tools_kwargs")
        if "interaction_kwargs" in inputs[0]:  # interaction args
            batch_gen_keys.append("interaction_kwargs")
        if "agent_name" in inputs[0]:  # agent name
            batch_gen_keys.append("agent_name")    
        if "env" in inputs[0]:  
            batch_gen_keys.append("env")
        
        # build generation batch
        gen_batch = InteractionDataProto()
        for key in batch_gen_keys:  
            gen_batch.no_tensor_batch[key] = [x[key] for x in inputs]
        
        prompts_text = [maybe_apply_chat_template(example, self.processing_class)["prompt"] for example in inputs]
        prompt_inputs = self.processing_class(
            text=prompts_text, return_tensors="pt", padding=True, padding_side="left", add_special_tokens=False
        )
               
        prompt_ids, prompt_mask = prompt_inputs["input_ids"], prompt_inputs["attention_mask"]
        if self.max_prompt_length is not None:
            prompt_ids = prompt_ids[:, -self.max_prompt_length :]
            prompt_mask = prompt_mask[:, -self.max_prompt_length :]

        gen_batch.batch["input_ids"] = prompt_ids.to(device) 
        gen_batch.batch["attention_mask"] = prompt_mask.to(device)

        # Generate completions using either vLLM or regular generation
        if self.use_vllm:
            # First, update the vLLM weights if needed
            if self.state.global_step != self._last_loaded_step:
                self._move_model_to_vllm()
                self._last_loaded_step = self.state.global_step

            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process
            if self.vllm_mode == "server":
                all_prompts_text = gather_object(prompts_text)
                if self.accelerator.is_main_process:
                    # Since 'prompts' contains 'num_generations' duplicates, we first take unique prompts, and generate
                    # num_generations outputs for each one. This is faster than generating outputs for each duplicate
                    # prompt individually.
                    ordered_set_of_prompts = all_prompts_text[:: self.num_generations]
                    with profiling_context(self, "vLLM.generate"):
                        completion_ids = self.vllm_client.generate(
                            prompts=ordered_set_of_prompts,
                            n=self.num_generations,
                            repetition_penalty=self.repetition_penalty,
                            temperature=self.temperature,
                            top_p=self.top_p,
                            top_k=-1 if self.top_k is None else self.top_k,
                            min_p=0.0 if self.min_p is None else self.min_p,
                            max_tokens=self.max_completion_length,
                            guided_decoding_regex=self.guided_decoding_regex,
                        )
                else:
                    completion_ids = [None] * len(all_prompts_text)
                # Broadcast the completions from the main process to all processes, ensuring each process receives its
                # corresponding slice.
                completion_ids = broadcast_object_list(completion_ids, from_process=0)
                process_slice = slice(
                    self.accelerator.process_index * len(prompts),
                    (self.accelerator.process_index + 1) * len(prompts),
                )
                completion_ids = completion_ids[process_slice]

            # Generate completions using colocated vLLM instances: each device holds vLLM copy and work on their own batch of prompts
            elif self.vllm_mode == "colocate":
                if self.guided_decoding_regex:
                    guided_decoding = GuidedDecodingParams(backend="outlines", regex=self.guided_decoding_regex)
                else:
                    guided_decoding = None
                sampling_params = SamplingParams(
                    n=1,  # vLLM on each GPU generates only 1 in colocate mode
                    repetition_penalty=self.repetition_penalty,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    top_k=-1 if self.top_k is None else self.top_k,
                    min_p=0.0 if self.min_p is None else self.min_p,
                    max_tokens=self.max_completion_length,
                    guided_decoding=guided_decoding,
                )

                if self.vllm_tensor_parallel_size > 1:
                    # Gather prompts from all ranks in the TP group and flatten.
                    # Each rank starts with its own prompts; after gathering, all ranks see the full group set.
                    orig_size = len(prompts_text)
                    gathered_prompts = [None for _ in range(self.vllm_tensor_parallel_size)]
                    torch.distributed.all_gather_object(gathered_prompts, prompts_text, group=self.tp_group)
                    all_prompts_text = [p for sublist in gathered_prompts for p in sublist]
                else:
                    all_prompts_text = prompts_text

                with profiling_context(self, "vLLM.generate"):
                    all_outputs = self.llm.generate(all_prompts_text, sampling_params=sampling_params, use_tqdm=False)

                completion_ids = [output.token_ids for outputs in all_outputs for output in outputs.outputs]

                if self.vllm_tensor_parallel_size > 1:
                    # Slice completions for this rank within its TP group.
                    # Each rank generates all outputs — we keep only our share.
                    local_rank_in_group = torch.distributed.get_rank(group=self.tp_group)
                    tp_slice = slice(local_rank_in_group * orig_size, (local_rank_in_group + 1) * orig_size)
                    completion_ids = completion_ids[tp_slice]

            # Pad the completions, and concatenate them with the prompts
            completion_ids = [torch.tensor(ids, device=device) for ids in completion_ids]
            completion_ids = pad(completion_ids, padding_value=self.processing_class.pad_token_id)
            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        else:
            # Regular generation path
            with unwrap_model_for_generation(
                self.model_wrapped, self.accelerator, gather_deepspeed3_params=self.args.ds3_gather_for_generation
            ) as unwrapped_model:
                with (
                    FSDP.summon_full_params(self.model_wrapped, recurse=False)
                    if self.is_fsdp_enabled
                    else nullcontext()
                ):
                    prompt_ids = gen_batch.batch["input_ids"]
                    prompt_mask = gen_batch.batch["attention_mask"]
                    prompt_completion_ids, augmentation_ids = unwrapped_model.generate(
                        prompt_ids, prompt_mask, generation_config=self.generation_config, return_augmentation_mask=True
                    )
                    # Compute prompt length and extract completion ids
                    prompt_length = prompt_ids.size(1)
                    prompt_ids = prompt_completion_ids[:, :prompt_length]
                    completion_ids = prompt_completion_ids[:, prompt_length:]
        
        # Clear memory after generation (inference only, no gradients needed)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Mask everything after the first EOS token
        # is_eos = completion_ids == self.processing_class.eos_token_id
        # eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)
        # eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]
        # sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)
        # completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()
        # completion_ids = completion_ids * completion_mask
        completion_ids = self.tensor_fn.erase_after_first_eos(completion_ids, self.processing_class.eos_token_id)
        completion_mask = completion_ids != self.processing_class.eos_token_id
        is_eos = completion_ids == self.processing_class.eos_token_id
        # augmentation_mask: All sampled positions, not necessarily the ones enhanced by the weaver.
        augmentation_mask = completion_mask * (augmentation_ids != invalid_augmentation_id)
        
        # Convert tensor to a list of lists of token IDs. This will be passed to the reward function, avoiding the need
        # to re-tokenize completions if the reward is computed from tokens.
        completion_ids_list = [
            [id.item() for id, m in zip(row, mask_row) if m] for row, mask_row in zip(completion_ids, completion_mask)
        ]
        
        # Sum along sequence dimension (dim=1) to get completion length per sequence, used for logging
        completion_lengths = completion_mask.sum(1)
        augmentation_lengths = (augmentation_mask * augmentation_ids).sum(dim=1)

        # If a truncation-based output strategy is used, 
        # then for any sequence that has not generated an EOS token, its loss will be ignored during computation.
        if self.mask_truncated_completions:
            truncated_completions = ~is_eos.any(dim=1)
            completion_mask = completion_mask * (~truncated_completions).unsqueeze(1).int()
            augmentation_mask = augmentation_mask * (~truncated_completions).unsqueeze(1).int()
        
        # Concatenate prompt_mask with completion_mask for logit computation
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B, P + C)
        
        with torch.no_grad():
            # When using num_iterations == 1 and steps_per_generation <= gradient_accumulation_steps
            # old_per_token_logps == per_token_logps, so we can skip it's computation here, and use
            # per_token_logps.detach() instead.
            if self.num_iterations > 1 or self.args.steps_per_generation > self.args.gradient_accumulation_steps:
                old_per_token_logps = self._get_per_token_logps( 
                    self.model.trigger, prompt_completion_ids, attention_mask, augmentation_ids, augmentation_mask
                )
            else:
                old_per_token_logps = None
            
            # Compute the per-token log probabilities for the reference model
            if self.beta != 0.0:
                if self.ref_model is not None:
                    ref_per_token_logps = self._get_per_token_logps(
                        self.ref_model, prompt_completion_ids, attention_mask, augmentation_ids, augmentation_mask
                    )
                else:
                    with self.accelerator.unwrap_model(self.model).disable_adapter():
                        ref_per_token_logps = self._get_per_token_logps(
                            self.model.trigger, prompt_completion_ids, attention_mask, augmentation_ids, augmentation_mask
                        )
            else: 
                ref_per_token_logps = None
        
        # Clear memory after all inference computations (generation + old/ref logps)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Decode the generated completions
        completions_text = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)
        completions = completions_text
        
        # Log completions to grpo_logs.txt
        if self.accelerator.is_main_process:
            try:
                with open(self.grpo_log_file, "a", encoding="utf-8") as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"Step: {self.state.global_step} | Mode: {mode}\n")
                    f.write(f"{'='*80}\n")
                    for idx, (prompt_txt, completion_txt) in enumerate(zip(prompts, completions_text)):
                        f.write(f"\n[Sample {idx}]\n")
                        f.write(f"Prompt: {prompt_txt}\n")
                        f.write(f"Completion: {completion_txt}\n")
                        f.write(f"{'-'*80}\n")
                    f.flush()
            except Exception as e:
                import logging
                logging.warning(f"Failed to write to GRPO log file: {e}")

        for i in range(len(inputs)):   
            inputs[i]["augmentation_ids"] = augmentation_ids[i]
            inputs[i]["augmentation_mask"] = augmentation_mask[i]

        rewards_per_func = self._calculate_rewards(inputs, prompts, completions, completion_ids_list)

        # Apply weights to each reward function's output and sum
        rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1)

        # Compute grouped-wise rewards
        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)
        is_std_zero = torch.isclose(std_grouped_rewards, torch.zeros_like(std_grouped_rewards))

        # Normalize the rewards to compute the advantages
        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
        advantages = rewards - mean_grouped_rewards
        if self.scale_rewards:
            advantages = advantages / (std_grouped_rewards + 1e-4)

        # Slice to keep only the local part of the data
        process_slice = slice(
            self.accelerator.process_index * len(prompts),
            (self.accelerator.process_index + 1) * len(prompts),
        )
        all_process_advantages = advantages.clone()  # keep the aggregated advantages for logging
        advantages = advantages[process_slice]

        # Log the metrics
        if mode == "train":
            self.state.num_input_tokens_seen += self.accelerator.gather(attention_mask.sum()).sum().item()
        self._metrics[mode]["num_tokens"] = [self.state.num_input_tokens_seen]

        # Log completion lengths, mean, min, max
        agg_completion_lengths = self.accelerator.gather(completion_lengths)
        self._metrics[mode]["completions/mean_length"].append(agg_completion_lengths.float().mean().item())
        self._metrics[mode]["completions/min_length"].append(agg_completion_lengths.float().min().item())
        self._metrics[mode]["completions/max_length"].append(agg_completion_lengths.float().max().item())

        # Log augmentation lengths, mean, min, max
        agg_augmentation_lengths = self.accelerator.gather(augmentation_lengths)
        self._metrics[mode]["augmentations/mean_length"].append(agg_augmentation_lengths.float().mean().item())
        self._metrics[mode]["augmentations/min_length"].append(agg_augmentation_lengths.float().min().item())
        self._metrics[mode]["augmentations/max_length"].append(agg_augmentation_lengths.float().max().item())

        # Identify sequences that terminated with EOS and log their lengths
        agg_terminated_with_eos = self.accelerator.gather(is_eos.any(dim=1))
        term_completion_lengths = agg_completion_lengths[agg_terminated_with_eos]
        clipped_completions_ratio = 1 - len(term_completion_lengths) / len(agg_completion_lengths)
        self._metrics[mode]["completions/clipped_ratio"].append(clipped_completions_ratio)
        if len(term_completion_lengths) == 0:  # edge case where no terminated sequences are found
            term_completion_lengths = torch.zeros(1, device=device)
        self._metrics[mode]["completions/mean_terminated_length"].append(term_completion_lengths.float().mean().item())
        self._metrics[mode]["completions/min_terminated_length"].append(term_completion_lengths.float().min().item())
        self._metrics[mode]["completions/max_terminated_length"].append(term_completion_lengths.float().max().item())
        
        # Calculate mean reward per function, but only for samples where the function was applied (non-NaN values)
        for i, reward_func_name in enumerate(self.reward_func_names):
            mean_rewards = torch.nanmean(rewards_per_func[:, i]).item()
            self._metrics[mode][f"rewards/{reward_func_name}/mean"].append(mean_rewards)
            std_rewards = nanstd(rewards_per_func[:, i]).item()
            self._metrics[mode][f"rewards/{reward_func_name}/std"].append(std_rewards)
        self._metrics[mode]["reward"].append(mean_grouped_rewards.mean().item())
        self._metrics[mode]["reward_std"].append(std_grouped_rewards.mean().item())
        self._metrics[mode]["frac_reward_zero_std"].append(is_std_zero.float().mean().item())

        # Log prompt and completion texts
        self._logs["prompt"].extend(gather_object(prompts_text))
        self._logs["completion"].extend(gather_object(completions_text))
        for i, name in enumerate(self.reward_func_names):
            self._logs["rewards"][name].extend(rewards_per_func[:, i].tolist())
        self._logs["advantages"].extend(all_process_advantages.tolist())

        return {
            "prompt_ids": prompt_ids,
            "prompt_mask": prompt_mask,
            "completion_ids": completion_ids,
            "completion_mask": completion_mask,
            "augmentation_ids": augmentation_ids,
            "augmentation_mask": augmentation_mask,
            "advantages": advantages,
            "old_per_token_logps": old_per_token_logps,
            "ref_per_token_logps": ref_per_token_logps,
        }

    def _compute_loss(self, model, inputs):
        # Compute the per-token log probabilities for the model
        prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
        completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
        augmentation_ids, augmentation_mask = inputs["augmentation_ids"], inputs["augmentation_mask"]
        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)

        per_token_logps = self._get_per_token_logps(model.trigger, input_ids, attention_mask, augmentation_ids, augmentation_mask)
        
        # Compute the KL divergence between the model and the reference model
        if self.beta != 0.0:
            ref_per_token_logps = inputs["ref_per_token_logps"]
            per_token_kl = (
                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
            )

        # Compute the loss
        advantages = inputs["advantages"]
        # When using num_iterations == 1 and steps_per_generation <= gradient_accumulation_steps
        # old_per_token_logps == per_token_logps, so we can skip it's computation
        # (see _generate_and_score_completions) and use per_token_logps.detach() instead.
        old_per_token_logps = (
            per_token_logps.detach() if inputs["old_per_token_logps"] is None else inputs["old_per_token_logps"]
        )
        coef_1 = torch.exp(per_token_logps - old_per_token_logps)
        coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)

        # Two-sided clipping
        if self.args.delta is not None:
            coef_1 = torch.clamp(coef_1, max=self.args.delta)

        per_token_loss1 = coef_1 * advantages.unsqueeze(1)
        per_token_loss2 = coef_2 * advantages.unsqueeze(1)
        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
        if self.beta != 0.0:
            per_token_loss = per_token_loss + self.beta * per_token_kl

        if self.loss_type == "grpo":
            loss = ((per_token_loss * augmentation_mask).sum(-1) / augmentation_mask.sum(-1).clamp(min=1.0)).mean()
        elif self.loss_type == "bnpo":
            loss = (per_token_loss * augmentation_mask).sum() / augmentation_mask.sum().clamp(min=1.0)
        elif self.loss_type == "dr_grpo":
            loss = (per_token_loss * augmentation_mask).sum() / (augmentation_mask.size(0) * self.max_completion_length)
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")

        # Log the metrics
        mode = "train" if self.model.training else "eval"

        if self.beta != 0.0:
            mean_kl = (per_token_kl * augmentation_mask).sum() / augmentation_mask.sum()
            self._metrics[mode]["kl"].append(self.accelerator.gather(mean_kl).nanmean().item())

        # Compute the clipped probability ratios
        is_low_clipped = (coef_1 < 1 - self.epsilon_low) & (advantages.unsqueeze(1) < 0)
        is_high_clipped = (coef_1 > 1 + self.epsilon_high) & (advantages.unsqueeze(1) > 0)
        is_region_clipped = is_low_clipped | is_high_clipped

        low_clip = (is_low_clipped * augmentation_mask).sum() / augmentation_mask.sum()
        high_clip = (is_high_clipped * augmentation_mask).sum() / augmentation_mask.sum()
        clip_ratio = (is_region_clipped * augmentation_mask).sum() / augmentation_mask.sum()

        gathered_low_clip = self.accelerator.gather(low_clip)
        self._metrics[mode]["clip_ratio/low_mean"].append(gathered_low_clip.nanmean().item())
        self._metrics[mode]["clip_ratio/low_min"].append(nanmin(gathered_low_clip).item())
        gathered_high_clip = self.accelerator.gather(high_clip)
        self._metrics[mode]["clip_ratio/high_mean"].append(gathered_high_clip.nanmean().item())
        self._metrics[mode]["clip_ratio/high_max"].append(nanmax(gathered_high_clip).item())
        gathered_clip_ratio = self.accelerator.gather(clip_ratio)
        self._metrics[mode]["clip_ratio/region_mean"].append(gathered_clip_ratio.nanmean().item())
        return loss