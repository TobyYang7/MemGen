{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "MemGen Training",
            "type": "python",
            "request": "launch",
            "module": "accelerate.commands.launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "python": "${workspaceFolder}/.venv/bin/python",
            "env": {
                "DEBUG_MODE": "true",
                "LOG_PATH": "./test_output/debug_log_gsm8k.txt",
                "CUDA_VISIBLE_DEVICES": "0",
                "MAIN_PROCESS_PORT": "29507",
                "NCCL_DEBUG": "INFO",
                "NCCL_IB_DISABLE": "1",
                "NCCL_P2P_DISABLE": "1",
                "NCCL_ASYNC_DISABLE": "1"
            },
            "args": [
                "--config_file=configs/zero2.yaml",
                "main.py",
                "--cfg-path", "configs/latent_memory/gsm8k.yaml",
                "--options",
                "model.reasoner_model_name", "Qwen/Qwen2.5-1.5B-Instruct",
                "model.weaver.weaver_model_name", "Qwen/Qwen2.5-1.5B-Instruct",
                "model.trigger.trigger_model_name", "null",
                "model.weaver.prompt_latents_len", "8",
                "model.weaver.inference_latents_len", "8",
                "model.max_prompt_aug_num", "1",
                "model.max_inference_aug_num", "5",
                "model.load_model_path", "null",
                "datasets.gsm8k.mode", "sft",
                "run.mode", "train",
                "run.train_weaver", "True",
                "run.train_trigger", "False",
                "run.train_weaver_method", "sft",
                "run.generation.do_sample", "True",
                "run.generation.temperature", "1.0",
                "run.generation.max_response_length", "2048",
                "run.output_dir", "${workspaceFolder}/test_output/gsm8k"
            ]
        }
        ,
        {
            "name": "MemGen VIS Training",
            "type": "python",
            "request": "launch",
            "module": "accelerate.commands.launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "python": "/root/MemGen/.venv/bin/python",
            "env": {
                "DEBUG_MODE": "true",
                "LOG_PATH": "./test_output/debug_log_mm_math.txt",
                "CUDA_VISIBLE_DEVICES": "0",
                "MAIN_PROCESS_PORT": "29507",
                "NCCL_DEBUG": "WARN",
                "NCCL_IB_DISABLE": "1",
                "NCCL_P2P_DISABLE": "0",
                "NCCL_ASYNC_DISABLE": "1",
                "TORCH_DISTRIBUTED_DEBUG": "OFF"
            },
            "args": [
                "--num_processes=1",
                "--main_process_port=29507",
                "--config_file=configs/zero2.yaml",
                "main.py",
                "--cfg-path", "configs/latent_memory/mm_math.yaml",
                "--options",
                "model.reasoner_model_name", "Qwen/Qwen2.5-VL-7B-Instruct",
                "model.weaver.weaver_model_name", "Qwen/Qwen2.5-1.5B-Instruct",
                "model.trigger.trigger_model_name", "null",
                "model.weaver.prompt_latents_len", "4",
                "model.weaver.inference_latents_len", "4",
                "model.max_prompt_aug_num", "1",
                "model.max_inference_aug_num", "3",
                "model.load_model_path", "null",
                "run.mode", "train",
                "run.train_weaver", "True",
                "run.train_trigger", "False",
                "run.train_weaver_method", "grpo",
                "run.generation.do_sample", "True",
                "run.generation.temperature", "1.0",
                "run.generation.max_response_length", "1024",
                "run.output_dir", "${workspaceFolder}/test_output/mm_math",
                "datasets.mm_math.mode", "grpo"
            ]
        },

        {
            "name": "MemGen VIS Training Multi",
            "type": "python",
            "request": "launch",
            "module": "accelerate.commands.launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "python": "/root/MemGen/.venv/bin/python",
            "env": {
                "DEBUG_MODE": "true",
                "LOG_PATH": "./test_output/debug_log_mm_math.txt",
                "CUDA_VISIBLE_DEVICES": "0,1,2,3,4,5,6,7",
                "MAIN_PROCESS_PORT": "29507",
                "NCCL_DEBUG": "WARN",
                "NCCL_IB_DISABLE": "1",
                "NCCL_P2P_DISABLE": "0",
                "NCCL_ASYNC_DISABLE": "1",
                "TORCH_DISTRIBUTED_DEBUG": "OFF"
            },
            "args": [
                "--num_processes=8",
                "--main_process_port=29507",
                "--config_file=configs/zero2.yaml",
                "main.py",
                "--cfg-path", "configs/latent_memory/mm_math.yaml",
                "--options",
                "model.reasoner_model_name", "Qwen/Qwen2.5-VL-7B-Instruct",
                "model.weaver.weaver_model_name", "Qwen/Qwen2.5-1.5B-Instruct",
                "model.trigger.trigger_model_name", "null",
                "model.weaver.prompt_latents_len", "4",
                "model.weaver.inference_latents_len", "4",
                "model.max_prompt_aug_num", "1",
                "model.max_inference_aug_num", "3",
                "model.load_model_path", "null",
                "run.mode", "train",
                "run.train_weaver", "True",
                "run.train_trigger", "False",
                "run.train_weaver_method", "grpo",
                "run.generation.do_sample", "True",
                "run.generation.temperature", "1.0",
                "run.generation.max_response_length", "1024",
                "run.output_dir", "${workspaceFolder}/test_output/mm_math",
                "datasets.mm_math.mode", "grpo"
            ]
        },
        {
            "name": "LatMem Inference",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/scripts/infer_latmem.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "python": "${workspaceFolder}/.venv/bin/python",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0"
            },
            "args": [
                "--cfg", "configs/latent_memory/mm_math.yaml",
                "--text", "Describe this image in detail.",
                "--image", "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "--max_new_tokens", "256",
                "--temperature", "1",
                "--do_sample",
                "--options",
                "model.max_prompt_aug_num", "0",
                "model.max_inference_aug_num", "0"
            ]
        }
    ]
}
